from selenium import webdriver
import datetime
import pandas as pd
import os
from openpyxl import *
from openpyxl.worksheet.datavalidation import DataValidation
from difflib import SequenceMatcher

##### Global Variable #####
#검색날짜 수정#
search_date_term = 2

data_for_write = []
start_date = None
end_date = None
filepath = None
browser = None

#포털사이트 검색 단어#
keywords = ['롯데중앙연구소', '롯데 식품','롯데제과','롯데푸드','롯데칠성','롯데주류','롯데지알에스','식품음료',
            '스낵과자','디저트','베이커리','아이스크림','생수','탄산수','탄산음료','주스','소주','맥주','우유'
            ,'패스트푸드','샐러드','커피','껌','초콜릿','스낵','과자','HMR','오리온','농심','해태제과','빙그레','아이시스','삼다수',
              '칠성사이다','코카콜라','펩시','클라우드','테라','하이트','처음처럼','참이슬','앤제리너스','롯데리아',
            '맥도날드','kfc','spc','버거킹','맘스터치','세븐일레븐','gs25','CU','편의점 식품','일본 식품','일본 편의점','해외 식품','식품안전']

#검색 기사 중복 제거#
similarity = 0.7

#필수 포함 단어 목록#
includings = ['식품','껌','초콜릿','스낵','과자','HMR','오리온','농심','해태제과','빙그레','아이시스','삼다수',
              '칠성사이다','코카콜라','펩시','클라우드','테라','하이트','처음처럼','참이슬','앤제리너스','롯데리아'
              ,'맥도날드','kfc','spc','버거킹','맘스터치','세븐일레븐','gs25','CU']

#제거 단어 목록#
excludings = ['개인정보']

dv_select_formula = '"V"'
##### Global Variable #####


def get_news(url, travel_xpaths, titles_xpath, dates_xpath, site, category):
    browser.get(url)
    for travel_xpath in travel_xpaths:
        browser.find_element_by_xpath(travel_xpath).click()
    titles = browser.find_elements_by_xpath(titles_xpath)
    dates = browser.find_elements_by_xpath(dates_xpath)
    for idx in range(len(dates)):
        google_count = 0
        naver_count = 0

        date = dates[idx].text
        if site == '네이버' and category == '뉴스':
            date = end_date
        if site == '구글' and category == '뉴스':
            date = dates[idx].get_attribute('datetime')[:10]

        if start_date <= date <= end_date:
            url = titles[idx].get_attribute('href')

            data_for_write.append([site, category, titles[idx].text, url, date])
            if site == '구글' and category == '뉴스':
                google_count = google_count + 1
            if site == '네이버' and category == '뉴스':
                naver_count = naver_count + 1
        if google_count >= 5:
            break
        if naver_count >= 20:
            break


if __name__ == '__main__':
    data_for_write.append(['Site', 'Category', 'Title', 'URL', 'Date'])

    start_date = (datetime.datetime.now() - datetime.timedelta(days=search_date_term - 1)).strftime('%Y-%m-%d')
    end_date = (datetime.datetime.now()).strftime('%Y-%m-%d')
    filepath = os.getcwd() + '\\Files\\' + end_date + '_푸드뉴스.xlsx'
    chromedriverpath = os.getcwd() + '\\chromedriver'

    browser = webdriver.Chrome(chromedriverpath)

    # ##### GOOGLE Start #####
    # for keyword in keywords:
    #     get_news(
    #         url='https://news.google.com/search?'
    #             'q=' + keyword + '%20when%3A' + str(search_date_term) + 'd&hl=ko&gl=KR&ceid=KR%3Ako',
    #         travel_xpaths=[],
    #         titles_xpath='//*[@id="yDmH0d"]/c-wiz/div/div[2]/div[2]/div/main/c-wiz/div[1]/div/div/article/h3/a',
    #         dates_xpath='//*[@id="yDmH0d"]/c-wiz/div/div[2]/div[2]/div/main/c-wiz/div[1]/div/div/article/div[2]/div/time',
    #         site='구글', category='뉴스'
    #     )
    # ##### GOOGLE End #####

    ##### NAVER Start #####
    for keyword in keywords:
        try:
            get_news(
                url='https://m.search.naver.com/search.naver?where=m_news&'
                    'query=' + keyword +
                    '&sm=mtb_tnw&sort=0&photo=0&field=0&'
                    'pd=3&ds=' + start_date.replace('-', '.') + '&de=' + end_date.replace('-', '.') +
                    '&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3A'
                    'from' + start_date.replace('-', '') + 'to' + end_date.replace('-', ''),
                travel_xpaths=[],
                titles_xpath='//*[@id="news_result_list"]/li/div/a',
                dates_xpath='//*[@id="news_result_list"]/li/div/div[1]/div[3]/span[1]',
                site='네이버', category='뉴스'
            )
        except:
            continue
    ##### NAVER End #####

    browser.close()

    dataframe_undecided = pd.DataFrame(data_for_write[1:], columns=data_for_write[0])
    dataframe_undecided = dataframe_undecided.drop_duplicates('URL', keep="first")
    dataframe_undecided = dataframe_undecided.drop_duplicates('Title', keep="first")
    for index in dataframe_undecided.index.values:
        max_similarity = 0.0
        parsed_dataframe = (dataframe_undecided.loc[:(index)]).drop(index=index)
        for compare in parsed_dataframe["Title"]:
            max_similarity = max(max_similarity, SequenceMatcher(None, compare, dataframe_undecided.loc[index]["Title"]).ratio())
        if max_similarity >= similarity:
            dataframe_undecided = dataframe_undecided.drop(index=index)
    dataframe_original = dataframe_undecided

    dataframe_included = pd.DataFrame([], columns=data_for_write[0])
    for including in includings:
        dataframe_included = dataframe_included.append(dataframe_undecided[dataframe_undecided['Title'].str.contains(including)])
        dataframe_undecided = dataframe_undecided[dataframe_undecided['Title'].str.contains(including) == False]

    dataframe_excluded = pd.DataFrame([], columns=data_for_write[0])
    for excluding in excludings:
        dataframe_excluded = dataframe_excluded.append(dataframe_undecided[dataframe_undecided['Title'].str.contains(excluding)])
        dataframe_undecided = dataframe_undecided[dataframe_undecided['Title'].str.contains(excluding) == False]

    dataframe_original = dataframe_original.values.tolist()
    dataframe_included = dataframe_included.values.tolist()
    dataframe_undecided = dataframe_undecided.values.tolist()
    dataframe_excluded = dataframe_excluded.values.tolist()

    excel = Workbook()

    excel.create_sheet("Original")
    excel["Original"]["A1"] = data_for_write[0][0]
    excel["Original"]["B1"] = data_for_write[0][1]
    excel["Original"]["C1"] = data_for_write[0][2]
    excel["Original"]["D1"] = data_for_write[0][3]
    excel["Original"]["E1"] = data_for_write[0][4]
    row_num = 2
    for original in dataframe_original:
        excel["Original"]["A" + str(row_num)] = original[0]
        excel["Original"]["B" + str(row_num)] = original[1]
        excel["Original"]["C" + str(row_num)] = original[2]
        excel["Original"]["D" + str(row_num)] = original[3]
        excel["Original"]["E" + str(row_num)] = original[4]
        row_num = row_num + 1

    excel.create_sheet("Included")
    excel["Included"]["A1"] = data_for_write[0][0]
    excel["Included"]["B1"] = data_for_write[0][1]
    excel["Included"]["C1"] = data_for_write[0][2]
    excel["Included"]["D1"] = data_for_write[0][3]
    excel["Included"]["E1"] = data_for_write[0][4]
    row_num = 2
    for included in dataframe_included:
        excel["Included"]["A" + str(row_num)] = included[0]
        excel["Included"]["B" + str(row_num)] = included[1]
        excel["Included"]["C" + str(row_num)] = included[2]
        excel["Included"]["D" + str(row_num)] = included[3]
        excel["Included"]["E" + str(row_num)] = included[4]
        row_num = row_num + 1

    excel.create_sheet("Undecided")
    excel["Undecided"]["A1"] = data_for_write[0][0]
    excel["Undecided"]["B1"] = data_for_write[0][1]
    excel["Undecided"]["C1"] = data_for_write[0][2]
    excel["Undecided"]["D1"] = data_for_write[0][3]
    excel["Undecided"]["E1"] = data_for_write[0][4]
    row_num = 2
    for undecided in dataframe_undecided:
        excel["Undecided"]["A" + str(row_num)] = undecided[0]
        excel["Undecided"]["B" + str(row_num)] = undecided[1]
        excel["Undecided"]["C" + str(row_num)] = undecided[2]
        excel["Undecided"]["D" + str(row_num)] = undecided[3]
        excel["Undecided"]["E" + str(row_num)] = undecided[4]
        row_num = row_num + 1

    excel.create_sheet("Excluded")
    excel["Excluded"]["A1"] = data_for_write[0][0]
    excel["Excluded"]["B1"] = data_for_write[0][1]
    excel["Excluded"]["C1"] = data_for_write[0][2]
    excel["Excluded"]["D1"] = data_for_write[0][3]
    excel["Excluded"]["E1"] = data_for_write[0][4]
    row_num = 2
    for excluded in dataframe_excluded:
        excel["Excluded"]["A" + str(row_num)] = excluded[0]
        excel["Excluded"]["B" + str(row_num)] = excluded[1]
        excel["Excluded"]["C" + str(row_num)] = excluded[2]
        excel["Excluded"]["D" + str(row_num)] = excluded[3]
        excel["Excluded"]["E" + str(row_num)] = excluded[4]
        row_num = row_num + 1

    excel.save(filepath)
    excel.close()

    excel = load_workbook(filepath)
    for sheetname in ["Included", "Undecided"]:
        excel[sheetname]["F1"] = "선택"
        excel[sheetname].column_dimensions['A'].width = 15.6
        excel[sheetname].column_dimensions['B'].width = 15.6
        excel[sheetname].column_dimensions['C'].width = 50.6
        excel[sheetname].column_dimensions['E'].width = 15.6
        excel[sheetname].column_dimensions['F'].width = 5.6

        excel[sheetname].freeze_panes = "A2"

        dv_select = DataValidation(type="list", formula1=dv_select_formula, allow_blank=True)

        row_num = 2
        while row_num <= excel[sheetname].max_row:
            dv_select.add(excel[sheetname]["F" + str(row_num)])

            row_num = row_num + 1

        excel[sheetname].add_data_validation(dv_select)

    excel.remove(excel['Sheet'])
    excel.save(filepath)
    excel.close()
